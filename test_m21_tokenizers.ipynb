{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import zlib\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files from Hook Theory dataset: 650\n"
     ]
    }
   ],
   "source": [
    "# root_dir = '/media/datadisk/datasets/hooktheory_xmls'\n",
    "root_dir = 'data/gjt_melodies/Library_melodies/'\n",
    "data_files = []\n",
    "\n",
    "# Walk through all subdirectories and files\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.xml') or file.endswith('.mxl'):\n",
    "            full_path = os.path.join(dirpath, file)\n",
    "            data_files.append(full_path)\n",
    "\n",
    "print('Total files from Hook Theory dataset:', len(data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stats\n",
    "stats = {}\n",
    "\n",
    "def compute_compression_rate(array: np.ndarray, compression_method=zlib.compress) -> float:\n",
    "    \"\"\"\n",
    "    Compute the compression rate of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The NumPy array to compress.\n",
    "        compression_method (callable): The compression method to use. \n",
    "                                       Default is `zlib.compress`.\n",
    "\n",
    "    Returns:\n",
    "        float: The compression rate (compressed size / original size).\n",
    "    \"\"\"\n",
    "    # Convert the array to bytes\n",
    "    array_bytes = array.tobytes()\n",
    "    \n",
    "    # Compress the byte representation\n",
    "    compressed_bytes = compression_method(array_bytes)\n",
    "    \n",
    "    # Compute sizes\n",
    "    original_size = len(array_bytes)\n",
    "    compressed_size = len(compressed_bytes)\n",
    "    \n",
    "    # Calculate compression rate\n",
    "    compression_rate = compressed_size / original_size\n",
    "\n",
    "    return compression_rate\n",
    "\n",
    "def initialize_stats(key, tokenizer):\n",
    "    stats[key] = {\n",
    "        'vocab_size': len(tokenizer.vocab),\n",
    "        'seq_lens': [],\n",
    "        'compression_rates': []\n",
    "    }\n",
    "# end initialize_stats\n",
    "\n",
    "def update_stats(key, toks):\n",
    "    for t in toks['ids']:\n",
    "        stats[key]['seq_lens'].append( len(t) )\n",
    "        stats[key]['compression_rates'].append( compute_compression_rate(np.array(t)) )\n",
    "    stats[key]['mean_len'] = np.mean(stats[key]['seq_lens'])\n",
    "    stats[key]['std_len'] = np.std(stats[key]['seq_lens'])\n",
    "    stats[key]['mean_compression'] = np.mean(stats[key]['compression_rates'])\n",
    "    stats[key]['std_compression'] = np.std(stats[key]['compression_rates'])\n",
    "# end update_stats\n",
    "\n",
    "def print_stats(key):\n",
    "    print('vocab_size: ', stats[key]['vocab_size'])\n",
    "    print('mean len: ', stats[key]['mean_len'])\n",
    "    print('std len: ', stats[key]['std_len'])\n",
    "    print('mean cr: ', stats[key]['mean_compression'])\n",
    "    print('std cr: ', stats[key]['std_compression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChordSymbolTokenizer_m21\n",
      "len(chordSymbolTokenizer.vocab):  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/650 [00:00<?, ?it/s]C:\\Users\\dimak\\AppData\\Roaming\\Python\\Python311\\site-packages\\music21\\stream\\base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "Processing Files: 100%|██████████| 650/650 [00:39<00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  58\n",
      "['<h>', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'G:min7', '<bar>', 'position_0x00', 'G#:maj7', 'position_1x50', 'G:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'G:min7', '<bar>', 'position_0x00', 'G#:maj7', 'position_1x50', 'G:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'C#:maj', 'position_1x50', 'D#:maj', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'C#:maj', 'position_1x50', 'D#:maj', '<bar>', 'position_0x00', 'F:min7', '</s>']\n",
      "[7, 6, 8, 241, 6, 8, 299, 6, 8, 327, 20, 299, 6, 8, 241, 6, 8, 241, 6, 8, 299, 6, 8, 327, 20, 299, 6, 8, 241, 6, 8, 175, 6, 8, 175, 6, 8, 117, 20, 175, 6, 8, 241, 6, 8, 175, 6, 8, 175, 6, 8, 117, 20, 175, 6, 8, 241, 3]\n",
      "vocab_size:  436\n",
      "mean len:  105.60307692307693\n",
      "std len:  34.296615528159485\n",
      "mean cr:  0.21026091050938642\n",
      "std cr:  0.047336737720226695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('ChordSymbolTokenizer_m21')\n",
    "chordSymbolTokenizer = ChordSymbolTokenizer()\n",
    "print('len(chordSymbolTokenizer.vocab): ', len(chordSymbolTokenizer.vocab))\n",
    "initialize_stats('ChordSymbolTokenizer', chordSymbolTokenizer)\n",
    "toks_cs = chordSymbolTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_cs['tokens'][0]))\n",
    "print(toks_cs['tokens'][0])\n",
    "print(toks_cs['ids'][0])\n",
    "update_stats('ChordSymbolTokenizer', toks_cs)\n",
    "print_stats('ChordSymbolTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootTypeTokenizer\n",
      "len(rootTypeTokenizer.vocab):  129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/650 [00:00<?, ?it/s]C:\\Users\\dimak\\AppData\\Roaming\\Python\\Python311\\site-packages\\music21\\stream\\base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "Processing Files: 100%|██████████| 650/650 [00:41<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  78\n",
      "['<h>', '<bar>', 'position_0x00', 'F', 'min7', '<bar>', 'position_0x00', 'G', 'min7', '<bar>', 'position_0x00', 'G#', 'maj7', 'position_1x50', 'G', 'min7', '<bar>', 'position_0x00', 'F', 'min7', '<bar>', 'position_0x00', 'F', 'min7', '<bar>', 'position_0x00', 'G', 'min7', '<bar>', 'position_0x00', 'G#', 'maj7', 'position_1x50', 'G', 'min7', '<bar>', 'position_0x00', 'F', 'min7', '<bar>', 'position_0x00', 'D#', 'maj', '<bar>', 'position_0x00', 'D#', 'maj', '<bar>', 'position_0x00', 'C#', 'maj', 'position_1x50', 'D#', 'maj', '<bar>', 'position_0x00', 'F', 'min7', '<bar>', 'position_0x00', 'D#', 'maj', '<bar>', 'position_0x00', 'D#', 'maj', '<bar>', 'position_0x00', 'C#', 'maj', 'position_1x50', 'D#', 'maj', '<bar>', 'position_0x00', 'F', 'min7', '</s>']\n",
      "[7, 6, 8, 93, 108, 6, 8, 95, 108, 6, 8, 96, 107, 20, 95, 108, 6, 8, 93, 108, 6, 8, 93, 108, 6, 8, 95, 108, 6, 8, 96, 107, 20, 95, 108, 6, 8, 93, 108, 6, 8, 91, 100, 6, 8, 91, 100, 6, 8, 89, 100, 20, 91, 100, 6, 8, 93, 108, 6, 8, 91, 100, 6, 8, 91, 100, 6, 8, 89, 100, 20, 91, 100, 6, 8, 93, 108, 3]\n",
      "vocab_size:  129\n",
      "mean len:  143.68769230769232\n",
      "std len:  47.76576826296714\n",
      "mean cr:  0.1798243846983939\n",
      "std cr:  0.04090717833010736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('RootTypeTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer()\n",
    "print('len(rootTypeTokenizer.vocab): ', len(rootTypeTokenizer.vocab))\n",
    "initialize_stats('RootTypeTokenizer', rootTypeTokenizer)\n",
    "toks_rt = rootTypeTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_rt['tokens'][0]))\n",
    "print(toks_rt['tokens'][0])\n",
    "print(toks_rt['ids'][0])\n",
    "update_stats('RootTypeTokenizer', toks_rt)\n",
    "print_stats('RootTypeTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PitchClassTokenizer\n",
      "len(pitchClassTokenizer.vocab):  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [00:37<00:00, 17.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  110\n",
      "['<h>', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', 'chord_pc_7', 'position_1x50', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', 'chord_pc_7', 'position_1x50', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_pc_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_pc_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '</s>']\n",
      "[7, 6, 8, 93, 96, 88, 91, 6, 8, 95, 98, 90, 93, 6, 8, 96, 88, 91, 95, 20, 95, 98, 90, 93, 6, 8, 93, 96, 88, 91, 6, 8, 93, 96, 88, 91, 6, 8, 95, 98, 90, 93, 6, 8, 96, 88, 91, 95, 20, 95, 98, 90, 93, 6, 8, 93, 96, 88, 91, 6, 8, 91, 95, 98, 6, 8, 91, 95, 98, 6, 8, 89, 93, 96, 20, 91, 95, 98, 6, 8, 93, 96, 88, 91, 6, 8, 91, 95, 98, 6, 8, 91, 95, 98, 6, 8, 89, 93, 96, 20, 91, 95, 98, 6, 8, 93, 96, 88, 91, 3]\n",
      "vocab_size:  100\n",
      "mean len:  223.54461538461538\n",
      "std len:  76.19775088699592\n",
      "mean cr:  0.150140531446534\n",
      "std cr:  0.03592985697771671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('PitchClassTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer()\n",
    "print('len(pitchClassTokenizer.vocab): ', len(pitchClassTokenizer.vocab))\n",
    "initialize_stats('PitchClassTokenizer', pitchClassTokenizer)\n",
    "toks_pc = pitchClassTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_pc['tokens'][0]))\n",
    "print(toks_pc['tokens'][0])\n",
    "print(toks_pc['ids'][0])\n",
    "update_stats('PitchClassTokenizer', toks_pc)\n",
    "print_stats('PitchClassTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootPCTokenizer\n",
      "len(rootPCTokenizer.vocab):  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [00:40<00:00, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  110\n",
      "['<h>', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_root_8', 'chord_pc_0', 'chord_pc_3', 'chord_pc_7', 'position_1x50', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_root_8', 'chord_pc_0', 'chord_pc_3', 'chord_pc_7', 'position_1x50', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '</s>']\n",
      "[7, 6, 8, 93, 108, 100, 103, 6, 8, 95, 110, 102, 105, 6, 8, 96, 100, 103, 107, 20, 95, 110, 102, 105, 6, 8, 93, 108, 100, 103, 6, 8, 93, 108, 100, 103, 6, 8, 95, 110, 102, 105, 6, 8, 96, 100, 103, 107, 20, 95, 110, 102, 105, 6, 8, 93, 108, 100, 103, 6, 8, 91, 107, 110, 6, 8, 91, 107, 110, 6, 8, 89, 105, 108, 20, 91, 107, 110, 6, 8, 93, 108, 100, 103, 6, 8, 91, 107, 110, 6, 8, 91, 107, 110, 6, 8, 89, 105, 108, 20, 91, 107, 110, 6, 8, 93, 108, 100, 103, 3]\n",
      "vocab_size:  112\n",
      "mean len:  223.54461538461538\n",
      "std len:  76.19775088699592\n",
      "mean cr:  0.16310171965956863\n",
      "std cr:  0.03980306538009121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('RootPCTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer()\n",
    "print('len(rootPCTokenizer.vocab): ', len(rootPCTokenizer.vocab))\n",
    "initialize_stats('RootPCTokenizer', rootPCTokenizer)\n",
    "toks_rpc = rootPCTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_rpc['tokens'][0]))\n",
    "print(toks_rpc['tokens'][0])\n",
    "print(toks_rpc['ids'][0])\n",
    "update_stats('RootPCTokenizer', toks_rpc)\n",
    "print_stats('RootPCTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTRootPCTokenizer\n",
      "len(gctRootPCTokenizer.vocab):  112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [00:53<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  110\n",
      "['<h>', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', '<bar>', 'position_0x00', 'chord_root_0', 'chord_pc_3', 'chord_pc_7', 'chord_pc_8', 'position_1x50', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', '<bar>', 'position_0x00', 'chord_root_0', 'chord_pc_3', 'chord_pc_7', 'chord_pc_8', 'position_1x50', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_1', 'chord_pc_5', 'chord_pc_8', 'position_1x50', 'chord_root_3', 'chord_pc_7', 'chord_pc_10', '<bar>', 'position_0x00', 'chord_root_5', 'chord_pc_8', 'chord_pc_0', 'chord_pc_3', '</s>']\n",
      "[7, 6, 8, 93, 108, 100, 103, 6, 8, 98, 102, 105, 107, 6, 8, 88, 103, 107, 108, 20, 98, 102, 105, 107, 6, 8, 93, 108, 100, 103, 6, 8, 93, 108, 100, 103, 6, 8, 98, 102, 105, 107, 6, 8, 88, 103, 107, 108, 20, 98, 102, 105, 107, 6, 8, 93, 108, 100, 103, 6, 8, 91, 107, 110, 6, 8, 91, 107, 110, 6, 8, 89, 105, 108, 20, 91, 107, 110, 6, 8, 93, 108, 100, 103, 6, 8, 91, 107, 110, 6, 8, 91, 107, 110, 6, 8, 89, 105, 108, 20, 91, 107, 110, 6, 8, 93, 108, 100, 103, 3]\n",
      "vocab_size:  112\n",
      "mean len:  223.54461538461538\n",
      "std len:  76.19775088699592\n",
      "mean cr:  0.1594067177317901\n",
      "std cr:  0.03846120659608798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('GCTRootPCTokenizer')\n",
    "gctRootPCTokenizer = GCTRootPCTokenizer()\n",
    "print('len(gctRootPCTokenizer.vocab): ', len(gctRootPCTokenizer.vocab))\n",
    "initialize_stats('GCTRootPCTokenizer', gctRootPCTokenizer)\n",
    "toks_gct_rpc = gctRootPCTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_gct_rpc['tokens'][0]))\n",
    "print(toks_gct_rpc['tokens'][0])\n",
    "print(toks_gct_rpc['ids'][0])\n",
    "update_stats('GCTRootPCTokenizer', toks_gct_rpc)\n",
    "print_stats('GCTRootPCTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTSymbolTokenizer\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [01:00<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gctSymbolTokenizer.vocab):  316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [01:31<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  58\n",
      "['<h>', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '<bar>', 'position_0x00', '[10  0  4  7  9]', '<bar>', 'position_0x00', '[0 0 3 7 8]', 'position_1x50', '[10  0  4  7  9]', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '<bar>', 'position_0x00', '[10  0  4  7  9]', '<bar>', 'position_0x00', '[0 0 3 7 8]', 'position_1x50', '[10  0  4  7  9]', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '<bar>', 'position_0x00', '[3 0 4 7]', '<bar>', 'position_0x00', '[3 0 4 7]', '<bar>', 'position_0x00', '[1 0 4 7]', 'position_1x50', '[3 0 4 7]', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '<bar>', 'position_0x00', '[3 0 4 7]', '<bar>', 'position_0x00', '[3 0 4 7]', '<bar>', 'position_0x00', '[1 0 4 7]', 'position_1x50', '[3 0 4 7]', '<bar>', 'position_0x00', '[ 5  0  3  7 10]', '</s>']\n",
      "[7, 6, 8, 88, 6, 8, 89, 6, 8, 90, 20, 89, 6, 8, 88, 6, 8, 88, 6, 8, 89, 6, 8, 90, 20, 89, 6, 8, 88, 6, 8, 91, 6, 8, 91, 6, 8, 92, 20, 91, 6, 8, 88, 6, 8, 91, 6, 8, 91, 6, 8, 92, 20, 91, 6, 8, 88, 3]\n",
      "vocab_size:  316\n",
      "mean len:  105.60307692307693\n",
      "std len:  34.296615528159485\n",
      "mean cr:  0.19727714920173547\n",
      "std cr:  0.04547579983829817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('GCTSymbolTokenizer')\n",
    "gctSymbolTokenizer = GCTSymbolTokenizer()\n",
    "print('training')\n",
    "gctSymbolTokenizer.fit( data_files )\n",
    "print('len(gctSymbolTokenizer.vocab): ', len(gctSymbolTokenizer.vocab))\n",
    "initialize_stats('GCTSymbolTokenizer', gctSymbolTokenizer)\n",
    "toks_gct_symb = gctSymbolTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_gct_symb['tokens'][0]))\n",
    "print(toks_gct_symb['tokens'][0])\n",
    "print(toks_gct_symb['ids'][0])\n",
    "update_stats('GCTSymbolTokenizer', toks_gct_symb)\n",
    "print_stats('GCTSymbolTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTRootTypeTokenizer\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [01:11<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gctRootTypeTokenizer.vocab):  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [01:42<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  78\n",
      "['<h>', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '<bar>', 'position_0x00', 'chord_root_10', '[0 4 7 9]', '<bar>', 'position_0x00', 'chord_root_0', '[0 3 7 8]', 'position_1x50', 'chord_root_10', '[0 4 7 9]', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '<bar>', 'position_0x00', 'chord_root_10', '[0 4 7 9]', '<bar>', 'position_0x00', 'chord_root_0', '[0 3 7 8]', 'position_1x50', 'chord_root_10', '[0 4 7 9]', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '<bar>', 'position_0x00', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_1', '[0 4 7]', 'position_1x50', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '<bar>', 'position_0x00', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_1', '[0 4 7]', 'position_1x50', 'chord_root_3', '[0 4 7]', '<bar>', 'position_0x00', 'chord_root_5', '[ 0  3  7 10]', '</s>']\n",
      "[7, 6, 8, 93, 100, 6, 8, 98, 101, 6, 8, 88, 102, 20, 98, 101, 6, 8, 93, 100, 6, 8, 93, 100, 6, 8, 98, 101, 6, 8, 88, 102, 20, 98, 101, 6, 8, 93, 100, 6, 8, 91, 103, 6, 8, 91, 103, 6, 8, 89, 103, 20, 91, 103, 6, 8, 93, 100, 6, 8, 91, 103, 6, 8, 91, 103, 6, 8, 89, 103, 20, 91, 103, 6, 8, 93, 100, 3]\n",
      "vocab_size:  145\n",
      "mean len:  143.68769230769232\n",
      "std len:  47.76576826296714\n",
      "mean cr:  0.1787124208468206\n",
      "std cr:  0.04103985131467332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('GCTRootTypeTokenizer')\n",
    "gctRootTypeTokenizer = GCTRootTypeTokenizer()\n",
    "print('training')\n",
    "gctRootTypeTokenizer.fit( data_files )\n",
    "print('len(gctRootTypeTokenizer.vocab): ', len(gctRootTypeTokenizer.vocab))\n",
    "initialize_stats('GCTRootTypeTokenizer', gctRootTypeTokenizer)\n",
    "toks_gct_rt = gctRootTypeTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_gct_rt['tokens'][0]))\n",
    "print(toks_gct_rt['tokens'][0])\n",
    "print(toks_gct_rt['ids'][0])\n",
    "update_stats('GCTRootTypeTokenizer', toks_gct_rt)\n",
    "print_stats('GCTRootTypeTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelodyPitchTokenizer_m21\n",
      "len(melodyPitchTokenizer.vocab):  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Melody Files: 100%|██████████| 650/650 [00:26<00:00, 24.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  78\n",
      "['<s>', '<bar>', 'position_0x00', 'P:65', 'position_1x00', 'P:72', 'position_2x50', 'P:68', '<bar>', 'position_0x00', 'P:70', '<bar>', 'position_0x00', 'P:68', 'position_1x50', 'P:70', '<bar>', 'position_0x00', 'P:65', 'position_2x00', '<rest>', 'position_2x50', 'P:60', '<bar>', 'position_0x00', 'P:65', 'position_1x00', 'P:72', 'position_2x50', 'P:68', '<bar>', 'position_0x00', 'P:70', '<bar>', 'position_0x00', 'P:68', 'position_1x50', 'P:70', '<bar>', 'position_0x00', 'P:65', '<bar>', 'position_0x00', 'P:63', 'position_1x00', 'P:63', 'position_2x00', 'P:67', '<bar>', 'position_0x00', 'P:63', '<bar>', 'position_0x00', 'P:61', 'position_1x50', 'P:63', '<bar>', 'position_0x00', 'P:65', '<bar>', 'position_0x00', 'P:63', 'position_1x00', 'P:63', 'position_2x00', 'P:67', '<bar>', 'position_0x00', 'P:63', '<bar>', 'position_0x00', 'P:61', 'position_1x50', 'P:68', '<bar>', 'position_0x00', 'P:65', '</s>']\n",
      "[2, 6, 95, 51, 104, 58, 118, 54, 6, 95, 56, 6, 95, 54, 109, 56, 6, 95, 51, 113, 4, 118, 46, 6, 95, 51, 104, 58, 118, 54, 6, 95, 56, 6, 95, 54, 109, 56, 6, 95, 51, 6, 95, 49, 104, 49, 113, 53, 6, 95, 49, 6, 95, 47, 109, 49, 6, 95, 51, 6, 95, 49, 104, 49, 113, 53, 6, 95, 49, 6, 95, 47, 109, 54, 6, 95, 51, 3]\n",
      "vocab_size:  185\n",
      "mean len:  243.8323076923077\n",
      "std len:  88.73966003938332\n",
      "mean cr:  0.19562656339567921\n",
      "std cr:  0.041858688716188884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('MelodyPitchTokenizer_m21')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer(min_pitch=21, max_pitch=108) #default range, need to adjust\n",
    "print('len(melodyPitchTokenizer.vocab): ', len(melodyPitchTokenizer.vocab))\n",
    "initialize_stats('MelodyPitchTokenizer', melodyPitchTokenizer)\n",
    "toks_cs = melodyPitchTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_cs['tokens'][0]))\n",
    "print(toks_cs['tokens'][0])\n",
    "print(toks_cs['ids'][0])\n",
    "update_stats('MelodyPitchTokenizer', toks_cs)\n",
    "print_stats('MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stats\n",
    "tokenizers = ['ChordSymbolTokenizer', 'MelodyPitchTokenizer'\n",
    "              ]\n",
    "\n",
    "results_path = 'vocab_stats_hk_m21.csv' #for hook theory\n",
    "\n",
    "result_fields = ['Tokenizer_m21', 'vocab_size'] + list( stats['ChordSymbolTokenizer'].keys() )[3:]\n",
    "\n",
    "with open( results_path, 'w' ) as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow( result_fields )\n",
    "\n",
    "for tok in tokenizers:\n",
    "    with open( results_path, 'a' ) as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow( [tok] + [stats[tok]['vocab_size']] + list( stats[tok].values() )[3:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging harmony vocab\n"
     ]
    }
   ],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer, verbose=1)\n",
    "#m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "#m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "#m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)\n",
    "#m_gctRootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, gctRootPCTokenizer)\n",
    "#m_gctSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, gctSymbolTokenizer)\n",
    "#m_gctRootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, gctRootTypeTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined vocab: 555\n",
      "Training melody tokenizer\n",
      "Merging melody vocab\n",
      "Training harmony tokenizer\n",
      "Merging harmony vocab\n",
      "Processing melody\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Melody Files:   0%|          | 0/650 [00:00<?, ?it/s]C:\\Users\\dimak\\AppData\\Roaming\\Python\\Python311\\site-packages\\music21\\stream\\base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "Processing Melody Files: 100%|██████████| 650/650 [00:24<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing harmony\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 650/650 [00:41<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence length:  136\n",
      "['<s>', '<bar>', 'position_0x00', 'P:65', 'position_1x00', 'P:72', 'position_2x50', 'P:68', '<bar>', 'position_0x00', 'P:70', '<bar>', 'position_0x00', 'P:68', 'position_1x50', 'P:70', '<bar>', 'position_0x00', 'P:65', 'position_2x00', '<rest>', 'position_2x50', 'P:60', '<bar>', 'position_0x00', 'P:65', 'position_1x00', 'P:72', 'position_2x50', 'P:68', '<bar>', 'position_0x00', 'P:70', '<bar>', 'position_0x00', 'P:68', 'position_1x50', 'P:70', '<bar>', 'position_0x00', 'P:65', '<bar>', 'position_0x00', 'P:63', 'position_1x00', 'P:63', 'position_2x00', 'P:67', '<bar>', 'position_0x00', 'P:63', '<bar>', 'position_0x00', 'P:61', 'position_1x50', 'P:63', '<bar>', 'position_0x00', 'P:65', '<bar>', 'position_0x00', 'P:63', 'position_1x00', 'P:63', 'position_2x00', 'P:67', '<bar>', 'position_0x00', 'P:63', '<bar>', 'position_0x00', 'P:61', 'position_1x50', 'P:68', '<bar>', 'position_0x00', 'P:65', '</s>', '<h>', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'G:min7', '<bar>', 'position_0x00', 'G#:maj7', 'position_1x50', 'G:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'G:min7', '<bar>', 'position_0x00', 'G#:maj7', 'position_1x50', 'G:min7', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'C#:maj', 'position_1x50', 'D#:maj', '<bar>', 'position_0x00', 'F:min7', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'D#:maj', '<bar>', 'position_0x00', 'C#:maj', 'position_1x50', 'D#:maj', '<bar>', 'position_0x00', 'F:min7', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fixing combined MergedMelHarmTokenizer\n",
    "print('Length of combined vocab:', len(m_chordSymbolTokenizer.vocab))\n",
    "print('Combined vocab:', m_chordSymbolTokenizer.vocab)\n",
    "\n",
    "m_chordSymbolTokenizer.fit( data_files )\n",
    "toks_symb_m = m_chordSymbolTokenizer(data_files)\n",
    "print('example sentence length: ', len(toks_symb_m['tokens'][0]))\n",
    "print(toks_symb_m['tokens'][0])\n",
    "print(toks_symb_m['ids'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 213]\n"
     ]
    }
   ],
   "source": [
    "print(m_chordSymbolTokenizer.convert_tokens_to_ids(['<mask>', 'C:7']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
