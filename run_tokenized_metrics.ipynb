{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "file_path = './data/results/tokenized/bart_reg_1.0/RootPCTokenizer.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Convert the 'real' and 'generated' columns to lists\n",
    "real_list = df[\"real\"].tolist()\n",
    "generated_list = df[\"generated\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(seq):\n",
    "    tokens = seq.split()\n",
    "    \n",
    "    # 1) Remove the leading <h> token if present\n",
    "    if tokens and tokens[0] == \"<h>\":\n",
    "        tokens.pop(0)  # remove <h>\n",
    "    \n",
    "    # 2) If the *first* token is <bar> and the *second* token is also <bar>, remove the second one\n",
    "    if len(tokens) > 1 and tokens[0] == \"<bar>\" and tokens[1] == \"<bar>\":\n",
    "        tokens.pop(1)  # remove the duplicate <bar>\n",
    "    \n",
    "    # Return the cleaned-up string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess both lists\n",
    "real_list = [preprocess_sequence(seq) for seq in real_list]\n",
    "generated_list = [preprocess_sequence(seq) for seq in generated_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 position_1x50 chord_root_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_7 chord_pc_11 chord_pc_2 position_1x50 chord_root_9 chord_pc_0 chord_pc_4 <bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 position_1x50 chord_root_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_7 chord_pc_11 chord_pc_2 position_1x50 chord_root_9 chord_pc_0 chord_pc_4 <bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 position_1x50 chord_root_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_7 chord_pc_11 chord_pc_2 position_1x50 chord_root_9 chord_pc_0 chord_pc_4 <bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 position_1x50 chord_root_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_9 chord_pc_0 chord_pc_4 position_1x50 chord_root_7 chord_pc_11 chord_pc_2 </s>\n",
      "<bar> position_0x00 chord_root_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_7 chord_pc_11 chord_pc_2 <bar> position_0x00 chord_root_9 chord_pc_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 chord_pc_4 position_3x00 chord_root_4 chord_pc_7 chord_pc_11 chord_pc_2 position_3x50 chord_root_9 chord_pc_0 chord_pc_4 chord_pc_7 <bar> position_0x00 chord_root_7 chord_pc_0 chord_pc_2 <bar> position_0x00 chord_root_5 chord_pc_9 chord_pc_0 chord_pc_4 <bar> position_0x00 chord_root_7 chord_pc_11 chord_pc_2 <bar> position_0x00 chord_root_0 chord_pc_4 chord_pc_7 </s>\n"
     ]
    }
   ],
   "source": [
    "gt_seq = real_list[5]\n",
    "print(gt_seq)\n",
    "pred_seq = generated_list[5]\n",
    "print(pred_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RootPCTokenizer\n",
    "def check_token_consistency(seq):\n",
    "    \"\"\"\n",
    "    Checks the consistency of token transitions based on predefined rules, \n",
    "    including cases where chords are represented with separate root and pitch class tokens.\n",
    "\n",
    "    Args:\n",
    "        seq (list): A list of tokens in the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: Consistency ratio (percentage of correct transitions).\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expression patterns for token types\n",
    "    position_pattern = re.compile(r'position_\\d+x\\d+')  # Matches position_AxBB\n",
    "    chord_root_pattern = re.compile(r'chord_root_\\d{1,2}')  # Matches chord root tokens like chord_root_3\n",
    "    pitch_class_pattern = re.compile(r'chord_pc_\\d{1,2}')  # Matches pitch class tokens like chord_pc_10\n",
    "\n",
    "    total_transitions = 0\n",
    "    valid_transitions = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(seq) - 1:\n",
    "        current_token = seq[i]\n",
    "        next_token = seq[i + 1]\n",
    "        total_transitions += 1\n",
    "\n",
    "        # Rule 1: <bar> must be followed by position_AxBB\n",
    "        if current_token == \"<bar>\":\n",
    "            if position_pattern.match(next_token):\n",
    "                valid_transitions += 1\n",
    "        # Rule 2: position_AxBB must be followed by chord_root_X\n",
    "        elif position_pattern.match(current_token):\n",
    "            if chord_root_pattern.match(next_token):\n",
    "                valid_transitions += 1\n",
    "        # Rule 3: chord_root_X must be followed by 3, 4, or 5 chord_pc_X tokens\n",
    "        elif chord_root_pattern.match(current_token):\n",
    "            chord_count = 0\n",
    "            while i + 1 < len(seq) and pitch_class_pattern.match(seq[i + 1]):\n",
    "                chord_count += 1\n",
    "                i += 1  # Move to the next chord_pc_X token\n",
    "            \n",
    "            if chord_count in {2, 3, 4}:\n",
    "                valid_transitions += 1  # Chord group size is valid\n",
    "        # Rule 4: chord_pc_X tokens must be followed by position_AxBB, <bar>, or </s>\n",
    "        elif pitch_class_pattern.match(current_token):\n",
    "            if position_pattern.match(next_token) or next_token in [\"<bar>\", \"</s>\"]:\n",
    "                valid_transitions += 1\n",
    "\n",
    "        i += 1  # Move to the next token\n",
    "\n",
    "    # Calculate consistency ratio as a percentage\n",
    "    consistency_ratio = (valid_transitions / total_transitions) * 100 if total_transitions > 0 else 100\n",
    "\n",
    "    return consistency_ratio\n",
    "\n",
    "\n",
    "\n",
    "def calculate_duplicate_error_ratio(seq):\n",
    "    \"\"\"\n",
    "    Calculate the total duplicate error ratio for the sequence.\n",
    "\n",
    "    Args:\n",
    "        seq (list): List of tokens in the sequence.\n",
    "\n",
    "    Returns:\n",
    "        float: The overall duplicate error ratio as a percentage.\n",
    "    \"\"\"\n",
    "    duplicate_count = 0\n",
    "    total_tokens = len(seq)\n",
    "\n",
    "    # Count consecutive duplicate tokens\n",
    "    for i in range(len(seq) - 1):\n",
    "        if seq[i] == seq[i + 1]:  # Consecutive duplicate found\n",
    "            duplicate_count += 1\n",
    "\n",
    "    # Calculate the total duplicate ratio as a percentage\n",
    "    duplicate_ratio = (duplicate_count / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "\n",
    "    return duplicate_ratio\n",
    "\n",
    "\n",
    "def count_bar_tokens(gt_seq, pred_seq):\n",
    "    # Count occurrences of '<bar>' in both sequences\n",
    "    gt_bar_count = gt_seq.count('<bar>')\n",
    "    pred_bar_count = pred_seq.count('<bar>')\n",
    "    \n",
    "    # Calculate the difference only if counts are not identical\n",
    "    bar_diff = abs(gt_bar_count - pred_bar_count) if gt_bar_count != pred_bar_count else 0\n",
    "\n",
    "    # Count exact matches \n",
    "    is_bar_count_correct = gt_bar_count == pred_bar_count\n",
    "\n",
    "    return bar_diff, is_bar_count_correct\n",
    "\n",
    "\n",
    "def low_level_metrics(gt_seqs, pred_seqs):\n",
    "    token_diff = []\n",
    "    bar_diff_list = []\n",
    "    num_no_eos = 0\n",
    "    num_bar_identical = 0\n",
    "    duplicate_error_ratios = []\n",
    "    consistency_token_ratios = []\n",
    "\n",
    "    prob_seqs = []\n",
    "\n",
    "    for i in range(0, len(gt_seqs)):\n",
    "\n",
    "        gt_seq = gt_seqs[i]\n",
    "        gt_seq = gt_seq.split()\n",
    "        pred_seq = pred_seqs[i] \n",
    "        pred_seq = pred_seq.split()\n",
    "\n",
    "    \n",
    "        # Get predicted sequence\n",
    "        try:\n",
    "            eos_idx_pred = pred_seq.index('</s>')  # Find the first occurrence of </s> in predictions\n",
    "            token_diff.append(len(gt_seq) - len(pred_seq))\n",
    "        except ValueError:\n",
    "            num_no_eos += 1  # Count sequences without </s>\n",
    "    \n",
    "\n",
    "        # Calculate bar token differences and matches\n",
    "        bar_diff, bar_match = count_bar_tokens(gt_seq, pred_seq)\n",
    "\n",
    "        if bar_match:\n",
    "            num_bar_identical += 1\n",
    "        else:\n",
    "            bar_diff_list.append(bar_diff)\n",
    "            prob_seqs.append(pred_seq)\n",
    "\n",
    "\n",
    "        # Calculate duplicate token error ratio\n",
    "        duplicate_ratio = calculate_duplicate_error_ratio(pred_seq)\n",
    "        duplicate_error_ratios.append(duplicate_ratio)\n",
    "\n",
    "        # Calculate token consistency ratio\n",
    "        consistency_ratio = check_token_consistency(pred_seq)\n",
    "        consistency_token_ratios.append(consistency_ratio)\n",
    "\n",
    "\n",
    "    return token_diff, num_no_eos, bar_diff_list, num_bar_identical, duplicate_error_ratios, consistency_token_ratios, prob_seqs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences without </s>: 0\n",
      "Number of sequences with identical bar counts: 679\n",
      "Total Token Length Differences: Mean = -14.66, Std = 72.07\n",
      "Bar Token Differences: Mean = 2.57, Std = 4.29\n",
      "Duplicate Token Error Ratio: Mean = 0.38, Std = 3.76\n",
      "Token Consistency Ratio: Mean = 98.91, Std = 5.71\n"
     ]
    }
   ],
   "source": [
    "token_diff, num_no_eos, bar_diff, num_bar_identical, duplicate_error_ratios, consistency_token_ratios, prob_seqs  = low_level_metrics(real_list, generated_list)\n",
    "\n",
    "print(f\"Sequences without </s>: {num_no_eos}\")\n",
    "print(f\"Number of sequences with identical bar counts: {num_bar_identical}\")\n",
    "token_diff_mean = np.mean(token_diff)\n",
    "token_diff_std = np.std(token_diff)\n",
    "print(f\"Total Token Length Differences: Mean = {token_diff_mean:.2f}, Std = {token_diff_std:.2f}\")\n",
    "bar_diff_mean = np.mean(bar_diff)\n",
    "bar_diff_std = np.std(bar_diff)\n",
    "print(f\"Bar Token Differences: Mean = {bar_diff_mean:.2f}, Std = {bar_diff_std:.2f}\")\n",
    "duplicate_error_mean = np.mean(duplicate_error_ratios)\n",
    "duplicate_error_std = np.std(duplicate_error_ratios)\n",
    "print(f\"Duplicate Token Error Ratio: Mean = {duplicate_error_mean:.2f}, Std = {duplicate_error_std:.2f}\")\n",
    "consistency_ratio_mean = np.mean(consistency_token_ratios)\n",
    "consistency_ratio_std = np.std(consistency_token_ratios)\n",
    "print(f\"Token Consistency Ratio: Mean = {consistency_ratio_mean:.2f}, Std = {consistency_ratio_std:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "aseq = prob_seqs[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
