{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy\n",
    "from harmony_tokenizers import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, \\\n",
    "    GCTRootPCTokenizer, GCTSymbolTokenizer, GCTRootTypeTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "gjt_path = '/media/maximos/9C33-6BBD/data/gjt_melodies/Library_melodies/'\n",
    "gjt_list = os.listdir(gjt_path)\n",
    "print(len(gjt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/650 [00:00<00:12, 50.34it/s]Exception ignored in: <function ZipFile.__del__ at 0x7073d23bc680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/maximos/miniconda/envs/midi/lib/python3.11/zipfile.py\", line 1874, in __del__\n",
      "    self.close()\n",
      "  File \"/home/maximos/miniconda/envs/midi/lib/python3.11/zipfile.py\", line 1892, in close\n",
      "    self._write_end_record()\n",
      "  File \"/home/maximos/miniconda/envs/midi/lib/python3.11/zipfile.py\", line 1986, in _write_end_record\n",
      "    self.fp.write(endrec)\n",
      "TypeError: string argument expected, got 'bytes'\n",
      "100%|██████████| 650/650 [00:14<00:00, 46.38it/s]\n"
     ]
    }
   ],
   "source": [
    "gjt_pieces = []\n",
    "for i in tqdm(range(len(gjt_list))):\n",
    "    g = muspy.read_musicxml(gjt_path + gjt_list[i])\n",
    "    gjt_pieces.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "print(len(gjt_pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stats\n",
    "stats = {}\n",
    "\n",
    "def compute_compression_rate(array: np.ndarray, compression_method=zlib.compress) -> float:\n",
    "    \"\"\"\n",
    "    Compute the compression rate of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        array (np.ndarray): The NumPy array to compress.\n",
    "        compression_method (callable): The compression method to use. \n",
    "                                       Default is `zlib.compress`.\n",
    "\n",
    "    Returns:\n",
    "        float: The compression rate (compressed size / original size).\n",
    "    \"\"\"\n",
    "    # Convert the array to bytes\n",
    "    array_bytes = array.tobytes()\n",
    "    \n",
    "    # Compress the byte representation\n",
    "    compressed_bytes = compression_method(array_bytes)\n",
    "    \n",
    "    # Compute sizes\n",
    "    original_size = len(array_bytes)\n",
    "    compressed_size = len(compressed_bytes)\n",
    "    \n",
    "    # Calculate compression rate\n",
    "    compression_rate = compressed_size / original_size\n",
    "\n",
    "    return compression_rate\n",
    "\n",
    "def initialize_stats(key, tokenizer):\n",
    "    stats[key] = {\n",
    "        'vocab_size': len(tokenizer.vocab),\n",
    "        'seq_lens': [],\n",
    "        'compression_rates': []\n",
    "    }\n",
    "# end initialize_stats\n",
    "\n",
    "def update_stats(key, toks):\n",
    "    for t in toks['ids']:\n",
    "        stats[key]['seq_lens'].append( len(t) )\n",
    "        stats[key]['compression_rates'].append( compute_compression_rate(np.array(t)) )\n",
    "    stats[key]['mean_len'] = np.mean(stats[key]['seq_lens'])\n",
    "    stats[key]['std_len'] = np.std(stats[key]['seq_lens'])\n",
    "    stats[key]['mean_compression'] = np.mean(stats[key]['compression_rates'])\n",
    "    stats[key]['std_compression'] = np.std(stats[key]['compression_rates'])\n",
    "# end update_stats\n",
    "\n",
    "def print_stats(key):\n",
    "    print('mean: ', stats[key]['mean_len'])\n",
    "    print('std: ', stats[key]['std_len'])\n",
    "    print('mean cr: ', stats[key]['mean_compression'])\n",
    "    print('std cr: ', stats[key]['std_compression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChordSymbolTokenizer\n",
      "len(chordSymbolTokenizer.vocab):  370\n",
      "example sentence length:  110\n",
      "['bar', 'position_0x0', 'G:min7', 'bar', 'position_0x0', 'C:7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'G:min7', 'bar', 'position_0x0', 'C:7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'E:min7', 'bar', 'position_0x0', 'A:7', 'bar', 'position_0x0', 'D:min7', 'bar', 'position_0x0', 'G:7', 'bar', 'position_0x0', 'C:maj7', 'bar', 'position_0x0', 'D:min7', 'position_2x0', 'G:7', 'bar', 'position_0x0', 'G:min7', 'position_0x0', 'C:7', 'position_0x0', 'F:maj', 'bar', 'position_0x0', 'C:7', 'bar', 'position_0x0', 'G:min7', 'bar', 'position_0x0', 'C:7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'G:min7', 'bar', 'position_0x0', 'C:7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'A:maj7', 'position_2x0', 'A#:maj7', 'bar', 'position_0x0', 'D:7', 'bar', 'position_0x0', 'G:min7', 'bar', 'position_0x0', 'A#:min6', 'position_2x0', 'A:7', 'bar', 'position_0x0', 'F:maj7', 'position_2x0', 'D:min7', 'bar', 'position_0x0', 'G:min7', 'position_2x0', 'C:7', 'bar', 'position_0x0', 'F:maj7', 'bar', 'position_0x0', 'F:maj7']\n",
      "[5, 6, 233, 5, 6, 28, 5, 6, 174, 5, 6, 174, 5, 6, 233, 5, 6, 28, 5, 6, 174, 5, 6, 174, 5, 6, 146, 5, 6, 289, 5, 6, 88, 5, 6, 231, 5, 6, 29, 5, 6, 88, 10, 231, 5, 6, 233, 6, 28, 6, 167, 5, 6, 28, 5, 6, 233, 5, 6, 28, 5, 6, 174, 5, 6, 174, 5, 6, 233, 5, 6, 28, 5, 6, 174, 5, 6, 174, 5, 6, 290, 10, 319, 5, 6, 86, 5, 6, 233, 5, 6, 323, 10, 289, 5, 6, 174, 10, 88, 5, 6, 233, 10, 28, 5, 6, 174, 5, 6, 174]\n",
      "mean:  120.49230769230769\n",
      "std:  43.32468583113862\n",
      "mean cr:  0.10353544639816045\n",
      "std cr:  0.02788528857857864\n"
     ]
    }
   ],
   "source": [
    "print('ChordSymbolTokenizer')\n",
    "chordSymbolTokenizer = ChordSymbolTokenizer()\n",
    "print('len(chordSymbolTokenizer.vocab): ', len(chordSymbolTokenizer.vocab))\n",
    "# stats['ChordSymbolTokenizer'] = {\n",
    "#     'vocab_size': len(chordSymbolTokenizer.vocab),\n",
    "#     'seq_lens': [],\n",
    "#     'compression_rates': []\n",
    "# }\n",
    "initialize_stats('ChordSymbolTokenizer', chordSymbolTokenizer)\n",
    "toks_cs = chordSymbolTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_cs['tokens'][0]))\n",
    "print(toks_cs['tokens'][0])\n",
    "print(toks_cs['ids'][0])\n",
    "update_stats('ChordSymbolTokenizer', toks_cs)\n",
    "# for t in toks_cs['tokens']:\n",
    "#     stats['ChordSymbolTokenizer']['seq_lens'].append( len(t) )\n",
    "# stats['ChordSymbolTokenizer']['mean_len'] = np.mean(stats['ChordSymbolTokenizer']['seq_lens'])\n",
    "# stats['ChordSymbolTokenizer']['std_len'] = np.std(stats['ChordSymbolTokenizer']['seq_lens'])\n",
    "print_stats('ChordSymbolTokenizer')\n",
    "# print('mean: ', stats['ChordSymbolTokenizer']['mean_len'])\n",
    "# print('std: ', stats['ChordSymbolTokenizer']['std_len'])\n",
    "# print('mean cr: ', stats['ChordSymbolTokenizer']['mean_compression'])\n",
    "# print('std cr: ', stats['ChordSymbolTokenizer']['std_compression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootTypeTokenizer\n",
      "len(rootTypeTokenizer.vocab):  63\n",
      "example sentence length:  149\n",
      "['bar', 'position_0x0', 'G', 'min7', 'bar', 'position_0x0', 'C', '7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'G', 'min7', 'bar', 'position_0x0', 'C', '7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'E', 'min7', 'bar', 'position_0x0', 'A', '7', 'bar', 'position_0x0', 'D', 'min7', 'bar', 'position_0x0', 'G', '7', 'bar', 'position_0x0', 'C', 'maj7', 'bar', 'position_0x0', 'D', 'min7', 'position_2x0', 'G', '7', 'bar', 'position_0x0', 'G', 'min7', 'position_0x0', 'C', '7', 'position_0x0', 'F', 'maj', 'bar', 'position_0x0', 'C', '7', 'bar', 'position_0x0', 'G', 'min7', 'bar', 'position_0x0', 'C', '7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'G', 'min7', 'bar', 'position_0x0', 'C', '7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'A', 'maj7', 'position_2x0', 'A#', 'maj7', 'bar', 'position_0x0', 'D', '7', 'bar', 'position_0x0', 'G', 'min7', 'bar', 'position_0x0', 'A#', 'min6', 'position_2x0', 'A', '7', 'bar', 'position_0x0', 'F', 'maj7', 'position_2x0', 'D', 'min7', 'bar', 'position_0x0', 'G', 'min7', 'position_2x0', 'C', '7', 'bar', 'position_0x0', 'F', 'maj7', 'bar', 'position_0x0', 'F', 'maj7']\n",
      "[5, 6, 29, 42, 5, 6, 22, 40, 5, 6, 27, 41, 5, 6, 27, 41, 5, 6, 29, 42, 5, 6, 22, 40, 5, 6, 27, 41, 5, 6, 27, 41, 5, 6, 26, 42, 5, 6, 31, 40, 5, 6, 24, 42, 5, 6, 29, 40, 5, 6, 22, 41, 5, 6, 24, 42, 10, 29, 40, 5, 6, 29, 42, 6, 22, 40, 6, 27, 34, 5, 6, 22, 40, 5, 6, 29, 42, 5, 6, 22, 40, 5, 6, 27, 41, 5, 6, 27, 41, 5, 6, 29, 42, 5, 6, 22, 40, 5, 6, 27, 41, 5, 6, 27, 41, 5, 6, 31, 41, 10, 32, 41, 5, 6, 24, 40, 5, 6, 29, 42, 5, 6, 32, 45, 10, 31, 40, 5, 6, 27, 41, 10, 24, 42, 5, 6, 29, 42, 10, 22, 40, 5, 6, 27, 41, 5, 6, 27, 41]\n",
      "mean:  164.8876923076923\n",
      "std:  60.33976621584019\n"
     ]
    }
   ],
   "source": [
    "print('RootTypeTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer()\n",
    "print('len(rootTypeTokenizer.vocab): ', len(rootTypeTokenizer.vocab))\n",
    "stats['RootTypeTokenizer'] = {\n",
    "    'vocab_size': len(rootTypeTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_rt = rootTypeTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_rt['tokens'][0]))\n",
    "print(toks_rt['tokens'][0])\n",
    "print(toks_rt['ids'][0])\n",
    "for t in toks_rt['tokens']:\n",
    "    stats['RootTypeTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['RootTypeTokenizer']['mean_len'] = np.mean(stats['RootTypeTokenizer']['seq_lens'])\n",
    "stats['RootTypeTokenizer']['std_len'] = np.std(stats['RootTypeTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['RootTypeTokenizer']['mean_len'])\n",
    "print('std: ', stats['RootTypeTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PitchClassTokenizer\n",
      "len(pitchClassTokenizer.vocab):  34\n",
      "example sentence length:  226\n",
      "['bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_11', 'chord_pc_2', 'bar', 'position_0x0', 'chord_pc_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_pc_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_11', 'bar', 'position_0x0', 'chord_pc_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'position_2x0', 'chord_pc_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_8', 'position_2x0', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_9', 'bar', 'position_0x0', 'chord_pc_2', 'chord_pc_6', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_pc_10', 'chord_pc_1', 'chord_pc_5', 'chord_pc_7', 'position_2x0', 'chord_pc_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'position_2x0', 'chord_pc_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_pc_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'position_2x0', 'chord_pc_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4']\n",
      "[5, 7, 30, 33, 25, 28, 5, 7, 23, 27, 30, 33, 5, 7, 28, 32, 23, 27, 5, 7, 28, 32, 23, 27, 5, 7, 30, 33, 25, 28, 5, 7, 23, 27, 30, 33, 5, 7, 28, 32, 23, 27, 5, 7, 28, 32, 23, 27, 5, 7, 27, 30, 34, 25, 5, 7, 32, 24, 27, 30, 5, 7, 25, 28, 32, 23, 5, 7, 30, 34, 25, 28, 5, 7, 23, 27, 30, 34, 5, 7, 25, 28, 32, 23, 11, 30, 34, 25, 28, 5, 7, 30, 33, 25, 28, 7, 23, 27, 30, 33, 7, 28, 32, 23, 5, 7, 23, 27, 30, 33, 5, 7, 30, 33, 25, 28, 5, 7, 23, 27, 30, 33, 5, 7, 28, 32, 23, 27, 5, 7, 28, 32, 23, 27, 5, 7, 30, 33, 25, 28, 5, 7, 23, 27, 30, 33, 5, 7, 28, 32, 23, 27, 5, 7, 28, 32, 23, 27, 5, 7, 32, 24, 27, 31, 11, 33, 25, 28, 32, 5, 7, 25, 29, 32, 23, 5, 7, 30, 33, 25, 28, 5, 7, 33, 24, 28, 30, 11, 32, 24, 27, 30, 5, 7, 28, 32, 23, 27, 11, 25, 28, 32, 23, 5, 7, 30, 33, 25, 28, 11, 23, 27, 30, 33, 5, 7, 28, 32, 23, 27, 5, 7, 28, 32, 23, 27]\n",
      "mean:  255.10615384615386\n",
      "std:  95.19571479838959\n"
     ]
    }
   ],
   "source": [
    "print('PitchClassTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer()\n",
    "print('len(pitchClassTokenizer.vocab): ', len(pitchClassTokenizer.vocab))\n",
    "stats['PitchClassTokenizer'] = {\n",
    "    'vocab_size': len(pitchClassTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_pc = pitchClassTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_pc['tokens'][0]))\n",
    "print(toks_pc['tokens'][0])\n",
    "print(toks_pc['ids'][0])\n",
    "for t in toks_pc['tokens']:\n",
    "    stats['PitchClassTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['PitchClassTokenizer']['mean_len'] = np.mean(stats['PitchClassTokenizer']['seq_lens'])\n",
    "stats['PitchClassTokenizer']['std_len'] = np.std(stats['PitchClassTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['PitchClassTokenizer']['mean_len'])\n",
    "print('std: ', stats['PitchClassTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RootPCTokenizer\n",
      "len(rootPCTokenizer.vocab):  46\n",
      "example sentence length:  226\n",
      "['bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_4', 'chord_pc_7', 'chord_pc_11', 'chord_pc_2', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_11', 'bar', 'position_0x0', 'chord_root_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'position_2x0', 'chord_root_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_8', 'position_2x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_9', 'bar', 'position_0x0', 'chord_root_2', 'chord_pc_6', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_1', 'chord_pc_5', 'chord_pc_7', 'position_2x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'position_2x0', 'chord_root_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_10', 'chord_pc_2', 'chord_pc_5', 'position_2x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_4']\n",
      "[5, 7, 30, 45, 37, 40, 5, 7, 23, 39, 42, 45, 5, 7, 28, 44, 35, 39, 5, 7, 28, 44, 35, 39, 5, 7, 30, 45, 37, 40, 5, 7, 23, 39, 42, 45, 5, 7, 28, 44, 35, 39, 5, 7, 28, 44, 35, 39, 5, 7, 27, 42, 46, 37, 5, 7, 32, 36, 39, 42, 5, 7, 25, 40, 44, 35, 5, 7, 30, 46, 37, 40, 5, 7, 23, 39, 42, 46, 5, 7, 25, 40, 44, 35, 11, 30, 46, 37, 40, 5, 7, 30, 45, 37, 40, 7, 23, 39, 42, 45, 7, 28, 44, 35, 5, 7, 23, 39, 42, 45, 5, 7, 30, 45, 37, 40, 5, 7, 23, 39, 42, 45, 5, 7, 28, 44, 35, 39, 5, 7, 28, 44, 35, 39, 5, 7, 30, 45, 37, 40, 5, 7, 23, 39, 42, 45, 5, 7, 28, 44, 35, 39, 5, 7, 28, 44, 35, 39, 5, 7, 32, 36, 39, 43, 11, 33, 37, 40, 44, 5, 7, 25, 41, 44, 35, 5, 7, 30, 45, 37, 40, 5, 7, 33, 36, 40, 42, 11, 32, 36, 39, 42, 5, 7, 28, 44, 35, 39, 11, 25, 40, 44, 35, 5, 7, 30, 45, 37, 40, 11, 23, 39, 42, 45, 5, 7, 28, 44, 35, 39, 5, 7, 28, 44, 35, 39]\n",
      "mean:  255.10615384615386\n",
      "std:  95.19571479838959\n"
     ]
    }
   ],
   "source": [
    "print('RootPCTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer()\n",
    "print('len(rootPCTokenizer.vocab): ', len(rootPCTokenizer.vocab))\n",
    "stats['RootPCTokenizer'] = {\n",
    "    'vocab_size': len(rootPCTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_rpc = rootPCTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_rpc['tokens'][0]))\n",
    "print(toks_rpc['tokens'][0])\n",
    "print(toks_rpc['ids'][0])\n",
    "for t in toks_rpc['tokens']:\n",
    "    stats['RootPCTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['RootPCTokenizer']['mean_len'] = np.mean(stats['RootPCTokenizer']['seq_lens'])\n",
    "stats['RootPCTokenizer']['std_len'] = np.std(stats['RootPCTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['RootPCTokenizer']['mean_len'])\n",
    "print('std: ', stats['RootPCTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTRootPCTokenizer\n",
      "len(gctRootPCTokenizer.vocab):  46\n",
      "example sentence length:  226\n",
      "['bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_4', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_2', 'bar', 'position_0x0', 'chord_root_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_4', 'chord_pc_7', 'chord_pc_11', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_2', 'position_2x0', 'chord_root_7', 'chord_pc_11', 'chord_pc_2', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'position_0x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_8', 'position_2x0', 'chord_root_2', 'chord_pc_5', 'chord_pc_9', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_2', 'chord_pc_6', 'chord_pc_9', 'chord_pc_0', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_1', 'chord_pc_5', 'chord_pc_7', 'position_2x0', 'chord_root_9', 'chord_pc_1', 'chord_pc_4', 'chord_pc_7', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'position_2x0', 'chord_root_5', 'chord_pc_9', 'chord_pc_0', 'chord_pc_2', 'bar', 'position_0x0', 'chord_root_10', 'chord_pc_2', 'chord_pc_5', 'chord_pc_7', 'position_2x0', 'chord_root_0', 'chord_pc_4', 'chord_pc_7', 'chord_pc_10', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5', 'bar', 'position_0x0', 'chord_root_9', 'chord_pc_0', 'chord_pc_4', 'chord_pc_5']\n",
      "[5, 7, 33, 37, 40, 42, 5, 7, 23, 39, 42, 45, 5, 7, 32, 35, 39, 40, 5, 7, 32, 35, 39, 40, 5, 7, 33, 37, 40, 42, 5, 7, 23, 39, 42, 45, 5, 7, 32, 35, 39, 40, 5, 7, 32, 35, 39, 40, 5, 7, 30, 46, 37, 39, 5, 7, 32, 36, 39, 42, 5, 7, 28, 44, 35, 37, 5, 7, 30, 46, 37, 40, 5, 7, 27, 42, 46, 35, 5, 7, 28, 44, 35, 37, 11, 30, 46, 37, 40, 5, 7, 33, 37, 40, 42, 7, 23, 39, 42, 45, 7, 28, 44, 35, 5, 7, 23, 39, 42, 45, 5, 7, 33, 37, 40, 42, 5, 7, 23, 39, 42, 45, 5, 7, 32, 35, 39, 40, 5, 7, 32, 35, 39, 40, 5, 7, 33, 37, 40, 42, 5, 7, 23, 39, 42, 45, 5, 7, 32, 35, 39, 40, 5, 7, 32, 35, 39, 40, 5, 7, 32, 36, 39, 43, 11, 25, 40, 44, 45, 5, 7, 25, 41, 44, 35, 5, 7, 33, 37, 40, 42, 5, 7, 33, 36, 40, 42, 11, 32, 36, 39, 42, 5, 7, 32, 35, 39, 40, 11, 28, 44, 35, 37, 5, 7, 33, 37, 40, 42, 11, 23, 39, 42, 45, 5, 7, 32, 35, 39, 40, 5, 7, 32, 35, 39, 40]\n",
      "mean:  255.10615384615386\n",
      "std:  95.19571479838959\n"
     ]
    }
   ],
   "source": [
    "print('GCTRootPCTokenizer')\n",
    "gctRootPCTokenizer = GCTRootPCTokenizer()\n",
    "print('len(gctRootPCTokenizer.vocab): ', len(gctRootPCTokenizer.vocab))\n",
    "stats['GCTRootPCTokenizer'] = {\n",
    "    'vocab_size': len(gctRootPCTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_gct_rpc = gctRootPCTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_gct_rpc['tokens'][0]))\n",
    "print(toks_gct_rpc['tokens'][0])\n",
    "print(toks_gct_rpc['ids'][0])\n",
    "for t in toks_gct_rpc['tokens']:\n",
    "    stats['GCTRootPCTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['GCTRootPCTokenizer']['mean_len'] = np.mean(stats['GCTRootPCTokenizer']['seq_lens'])\n",
    "stats['GCTRootPCTokenizer']['std_len'] = np.std(stats['GCTRootPCTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['GCTRootPCTokenizer']['mean_len'])\n",
    "print('std: ', stats['GCTRootPCTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTSymbolTokenizer\n",
      "training\n",
      "len(gctSymbolTokenizer.vocab):  210\n",
      "example sentence length:  110\n",
      "['bar', 'position_0x0', '[10  0  4  7  9]', 'bar', 'position_0x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'bar', 'position_0x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[7 0 4 7 9]', 'bar', 'position_0x0', '[ 9  0  4  7 10]', 'bar', 'position_0x0', '[5 0 4 7 9]', 'bar', 'position_0x0', '[ 7  0  4  7 10]', 'bar', 'position_0x0', '[4 0 3 7 8]', 'bar', 'position_0x0', '[5 0 4 7 9]', 'position_2x0', '[ 7  0  4  7 10]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'position_0x0', '[ 0  0  4  7 10]', 'position_0x0', '[5 0 4 7]', 'bar', 'position_0x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'bar', 'position_0x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'bar', 'position_0x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[ 9  0  4  7 11]', 'position_2x0', '[2 0 3 7 8]', 'bar', 'position_0x0', '[ 2  0  4  7 10]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'bar', 'position_0x0', '[10  0  3  7  9]', 'position_2x0', '[ 9  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'position_2x0', '[5 0 4 7 9]', 'bar', 'position_0x0', '[10  0  4  7  9]', 'position_2x0', '[ 0  0  4  7 10]', 'bar', 'position_0x0', '[9 0 3 7 8]', 'bar', 'position_0x0', '[9 0 3 7 8]']\n",
      "[5, 7, 22, 5, 7, 23, 5, 7, 24, 5, 7, 24, 5, 7, 22, 5, 7, 23, 5, 7, 24, 5, 7, 24, 5, 7, 25, 5, 7, 26, 5, 7, 27, 5, 7, 28, 5, 7, 29, 5, 7, 27, 11, 28, 5, 7, 22, 7, 23, 7, 30, 5, 7, 23, 5, 7, 22, 5, 7, 23, 5, 7, 24, 5, 7, 24, 5, 7, 22, 5, 7, 23, 5, 7, 24, 5, 7, 24, 5, 7, 31, 11, 32, 5, 7, 33, 5, 7, 22, 5, 7, 34, 11, 26, 5, 7, 24, 11, 27, 5, 7, 22, 11, 23, 5, 7, 24, 5, 7, 24]\n",
      "mean:  120.49230769230769\n",
      "std:  43.32468583113862\n"
     ]
    }
   ],
   "source": [
    "print('GCTSymbolTokenizer')\n",
    "gctSymbolTokenizer = GCTSymbolTokenizer()\n",
    "print('training')\n",
    "gctSymbolTokenizer.fit( gjt_pieces )\n",
    "print('len(gctSymbolTokenizer.vocab): ', len(gctSymbolTokenizer.vocab))\n",
    "stats['GCTSymbolTokenizer'] = {\n",
    "    'vocab_size': len(gctSymbolTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_gct_symb = gctSymbolTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_gct_symb['tokens'][0]))\n",
    "print(toks_gct_symb['tokens'][0])\n",
    "print(toks_gct_symb['ids'][0])\n",
    "for t in toks_gct_symb['tokens']:\n",
    "    stats['GCTSymbolTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['GCTSymbolTokenizer']['mean_len'] = np.mean(stats['GCTSymbolTokenizer']['seq_lens'])\n",
    "stats['GCTSymbolTokenizer']['std_len'] = np.std(stats['GCTSymbolTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['GCTSymbolTokenizer']['mean_len'])\n",
    "print('std: ', stats['GCTSymbolTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTRootTypeTokenizer\n",
      "training\n",
      "len(gctRootTypeTokenizer.vocab):  71\n",
      "example sentence length:  149\n",
      "['bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_7', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_9', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_5', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_7', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_4', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_5', '[0 4 7 9]', 'position_2x0', 'chord_root_7', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'position_0x0', 'chord_root_5', '[0 4 7]', 'bar', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[ 0  4  7 11]', 'position_2x0', 'chord_root_2', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_2', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_10', '[0 3 7 9]', 'position_2x0', 'chord_root_9', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'position_2x0', 'chord_root_5', '[0 4 7 9]', 'bar', 'position_0x0', 'chord_root_10', '[0 4 7 9]', 'position_2x0', 'chord_root_0', '[ 0  4  7 10]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]', 'bar', 'position_0x0', 'chord_root_9', '[0 3 7 8]']\n",
      "[5, 7, 33, 34, 5, 7, 23, 35, 5, 7, 32, 36, 5, 7, 32, 36, 5, 7, 33, 34, 5, 7, 23, 35, 5, 7, 32, 36, 5, 7, 32, 36, 5, 7, 30, 34, 5, 7, 32, 35, 5, 7, 28, 34, 5, 7, 30, 35, 5, 7, 27, 36, 5, 7, 28, 34, 11, 30, 35, 5, 7, 33, 34, 7, 23, 35, 7, 28, 37, 5, 7, 23, 35, 5, 7, 33, 34, 5, 7, 23, 35, 5, 7, 32, 36, 5, 7, 32, 36, 5, 7, 33, 34, 5, 7, 23, 35, 5, 7, 32, 36, 5, 7, 32, 36, 5, 7, 32, 38, 11, 25, 36, 5, 7, 25, 35, 5, 7, 33, 34, 5, 7, 33, 39, 11, 32, 35, 5, 7, 32, 36, 11, 28, 34, 5, 7, 33, 34, 11, 23, 35, 5, 7, 32, 36, 5, 7, 32, 36]\n",
      "mean:  164.8876923076923\n",
      "std:  60.33976621584019\n"
     ]
    }
   ],
   "source": [
    "print('GCTRootTypeTokenizer')\n",
    "gctRootTypeTokenizer = GCTRootTypeTokenizer()\n",
    "print('training')\n",
    "gctRootTypeTokenizer.fit( gjt_pieces )\n",
    "print('len(gctRootTypeTokenizer.vocab): ', len(gctRootTypeTokenizer.vocab))\n",
    "stats['GCTRootTypeTokenizer'] = {\n",
    "    'vocab_size': len(gctRootTypeTokenizer.vocab),\n",
    "    'seq_lens': []\n",
    "}\n",
    "toks_gct_rt = gctRootTypeTokenizer(gjt_pieces)\n",
    "print('example sentence length: ', len(toks_gct_rt['tokens'][0]))\n",
    "print(toks_gct_rt['tokens'][0])\n",
    "print(toks_gct_rt['ids'][0])\n",
    "for t in toks_gct_rt['tokens']:\n",
    "    stats['GCTRootTypeTokenizer']['seq_lens'].append( len(t) )\n",
    "stats['GCTRootTypeTokenizer']['mean_len'] = np.mean(stats['GCTRootTypeTokenizer']['seq_lens'])\n",
    "stats['GCTRootTypeTokenizer']['std_len'] = np.std(stats['GCTRootTypeTokenizer']['seq_lens'])\n",
    "print('mean: ', stats['GCTRootTypeTokenizer']['mean_len'])\n",
    "print('std: ', stats['GCTRootTypeTokenizer']['std_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer             \t vocab size \t mean_len\t std_len\n",
      "ChordSymbolTokenizer: \t \t370\t 120.49\t\t 43.32\n",
      "GCTSymbolTokenizer: \t \t210\t 120.49\t\t 43.32\n",
      "RootTypeTokenizer: \t \t63\t 164.89\t\t 60.34\n",
      "GCTRootTypeTokenizer: \t \t71\t 164.89\t\t 60.34\n",
      "PitchClassTokenizer: \t \t34\t 255.11\t\t 95.20\n",
      "RootPCTokenizer: \t \t46\t 255.11\t\t 95.20\n",
      "GCTRootPCTokenizer: \t \t46\t 255.11\t\t 95.20\n"
     ]
    }
   ],
   "source": [
    "# print stats\n",
    "tokenizers = ['ChordSymbolTokenizer', 'GCTSymbolTokenizer',\\\n",
    "              'RootTypeTokenizer', 'GCTRootTypeTokenizer',\\\n",
    "              'PitchClassTokenizer', 'RootPCTokenizer', 'GCTRootPCTokenizer'\n",
    "              ]\n",
    "print('Tokenizer             \\t', 'vocab size \\t', 'mean_len\\t', 'std_len')\n",
    "for tok in tokenizers:\n",
    "    m = stats[tok]['mean_len']\n",
    "    s = stats[tok]['std_len']\n",
    "    print( tok + ': \\t', '\\t'+str( stats[tok]['vocab_size'] )+'\\t', f'{m:.2f}\\t\\t', f'{s:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
