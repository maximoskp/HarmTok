{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_utils import MergedMelHarmDataset, PureGenCollator\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/media/maindisk/maximos/data/hooktheory_test'\n",
    "\n",
    "data_files = []\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.xml') or file.endswith('.mxl'):\n",
    "            full_path = os.path.join(dirpath, file)\n",
    "            data_files.append(full_path)\n",
    "print(len(data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    'ChordSymbolTokenizer': ChordSymbolTokenizer,\n",
    "    'RootTypeTokenizer': RootTypeTokenizer,\n",
    "    'PitchClassTokenizer': PitchClassTokenizer,\n",
    "    'RootPCTokenizer': RootPCTokenizer,\n",
    "    'GCTRootPCTokenizer': GCTRootPCTokenizer,\n",
    "    'GCTSymbolTokenizer': GCTSymbolTokenizer,\n",
    "    'GCTRootTypeTokenizer': GCTRootTypeTokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = 'ChordSymbolTokenizer'\n",
    "val_dir = root_dir\n",
    "batchsize = 16\n",
    "\n",
    "melody_tokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')\n",
    "harmony_tokenizer = tokenizers[tokenizer_name].from_pretrained('saved_tokenizers/' + tokenizer_name)\n",
    "\n",
    "tokenizer = MergedMelHarmTokenizer(melody_tokenizer, harmony_tokenizer)\n",
    "\n",
    "val_dataset = MergedMelHarmDataset(val_dir, tokenizer, max_length=512, return_harmonization_labels=True, num_bars=8)\n",
    "collator = PureGenCollator(tokenizer)\n",
    "\n",
    "valloader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_counter = 0\n",
    "# with tqdm(valloader, unit='batch') as tepoch:\n",
    "#     tepoch.set_description(f'run')\n",
    "#     for batch in tepoch:\n",
    "#         for b in batch['input_ids']:\n",
    "#             tmp_counter += 1\n",
    "# print(tmp_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n"
     ]
    }
   ],
   "source": [
    "with open('tokenized/gpt_1.0/ChordSymbolTokenizer.pickle', \"rb\") as input_file:\n",
    "    p = pickle.load(input_file)\n",
    "m = p['generated']\n",
    "print(len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n",
      "1520\n",
      "0       <h> <bar> position_0x00 C:maj <bar> position_0...\n",
      "1       <h> <bar> position_0x00 C:maj <bar> position_0...\n",
      "2       <h> <bar> position_0x00 G:maj <bar> position_0...\n",
      "3       <h> <bar> position_0x00 C:maj position_2x00 F:...\n",
      "4       <h> <bar> position_0x00 D:min position_3x50 C:...\n",
      "                              ...                        \n",
      "1515    <h> <bar> position_0x00 C:maj position_2x00 D:...\n",
      "1516    <h> <bar> position_0x00 F:maj position_1x50 F:...\n",
      "1517    <h> <bar> position_0x00 A:min <bar> position_0...\n",
      "1518    <h> <bar> position_0x00 F:maj position_2x00 C:...\n",
      "1519    <h> <bar> position_0x00 E:min <bar> position_0...\n",
      "Name: generated, Length: 1520, dtype: object\n"
     ]
    }
   ],
   "source": [
    "c = pd.read_csv( 'tokenized/gpt_0.8/ChordSymbolTokenizer.csv' )\n",
    "print(len(c['generated']))\n",
    "print(c['generated'].size)\n",
    "print(c['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_1.0 - ChordSymbolTokenizer : 1520\n",
      "gpt_1.0 - RootTypeTokenizer : 1520\n",
      "gpt_1.0 - PitchClassTokenizer : 1520\n",
      "gpt_1.0 - RootPCTokenizer : 1520\n",
      "gpt_0.8 - ChordSymbolTokenizer : 1520\n",
      "gpt_0.8 - RootTypeTokenizer : 1520\n",
      "gpt_0.8 - PitchClassTokenizer : 1520\n",
      "gpt_0.8 - RootPCTokenizer : 1520\n",
      "gpt_1.2 - ChordSymbolTokenizer : 1520\n",
      "gpt_1.2 - RootTypeTokenizer : 1520\n",
      "gpt_1.2 - PitchClassTokenizer : 1520\n",
      "gpt_1.2 - RootPCTokenizer : 1520\n",
      "bart_1.0 - ChordSymbolTokenizer : 1520\n",
      "bart_1.0 - RootTypeTokenizer : 1520\n",
      "bart_1.0 - PitchClassTokenizer : 1520\n",
      "bart_1.0 - RootPCTokenizer : 1520\n",
      "bart_0.8 - ChordSymbolTokenizer : 1520\n",
      "bart_0.8 - RootTypeTokenizer : 1520\n",
      "bart_0.8 - PitchClassTokenizer : 1520\n",
      "bart_0.8 - RootPCTokenizer : 1520\n",
      "bart_1.2 - ChordSymbolTokenizer : 1520\n",
      "bart_1.2 - RootTypeTokenizer : 1520\n",
      "bart_1.2 - PitchClassTokenizer : 1520\n",
      "bart_1.2 - RootPCTokenizer : 1520\n"
     ]
    }
   ],
   "source": [
    "tokenized_folders = ['gpt_1.0', 'gpt_0.8', 'gpt_1.2',\\\n",
    "                     'bart_1.0', 'bart_0.8', 'bart_1.2']\n",
    "tokenizer_names = ['ChordSymbolTokenizer', 'RootTypeTokenizer', \\\n",
    "              'PitchClassTokenizer', 'RootPCTokenizer']\n",
    "for tok_folder in tokenized_folders:\n",
    "    for tokenizer_name in tokenizer_names:\n",
    "        c = pd.read_csv( 'tokenized/' + tok_folder + '/' + \\\n",
    "                        tokenizer_name + '.csv' )\n",
    "        print(tok_folder, '-', tokenizer_name, ':', len(c['generated']))\n",
    "        assert len(c['generated']) == 1520, 'Error: ' + tokenizer_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
