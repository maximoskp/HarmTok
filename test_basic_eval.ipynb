{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_basic_eval(df, tokenizer_name='ChordSymbolTokenizer'):\n",
    "    total_pieces = len(df['labels'])\n",
    "\n",
    "    correct_bar_predictions = 0\n",
    "    total_bars = 0\n",
    "\n",
    "    correct_new_chord_predictions = 0\n",
    "    total_new_chord_predictions = 0\n",
    "\n",
    "    correct_position_predictions = 0\n",
    "    total_position_predictions = 0\n",
    "\n",
    "    correct_chord_predictions = 0\n",
    "    correct_root_predictions = 0\n",
    "    total_chord_predictions = 0\n",
    "\n",
    "    for p_i in range(total_pieces):\n",
    "        l = df['labels'].iloc[p_i]\n",
    "        p = df['predictions'].iloc[p_i]\n",
    "        # for each token that should have been predicted\n",
    "        l_split = l.split(' ')\n",
    "        p_split = p.split(' ')\n",
    "        # arm root and type for RootType tokenizers\n",
    "        arm_root = False\n",
    "        tmp_root = None\n",
    "        tmp_type = None\n",
    "        # keep a chord buffer to accumulate elements of chords for \n",
    "        # non single-word chord representations\n",
    "        l_chord_buffer = []\n",
    "        p_chord_buffer = []\n",
    "        i = 0\n",
    "        while i < len (l_split):\n",
    "            # how many bars were correctly predicted\n",
    "            if l_split[i] == '<bar>':\n",
    "                total_bars += 1\n",
    "                arm_root = False\n",
    "                if p_split[i] == '<bar>':\n",
    "                    correct_bar_predictions += 1\n",
    "            # how many new chords were correctly predicted\n",
    "            if 'position_' in l_split[i]:\n",
    "                total_new_chord_predictions += 1\n",
    "                arm_root = True\n",
    "                if 'position_' in p_split[i]:\n",
    "                    correct_new_chord_predictions += 1\n",
    "            # how many correct positions were predicted\n",
    "            if 'position_' in l_split[i]:\n",
    "                total_position_predictions += 1\n",
    "                if p_split[i] == l_split[i]:\n",
    "                    correct_position_predictions += 1\n",
    "            # how many exact chords and roots were predicted\n",
    "            if tokenizer_name == 'ChordSymbolTokenizer':\n",
    "                if ':' in l_split[i]:\n",
    "                    total_chord_predictions += 1\n",
    "                    if p_split[i] == l_split[i]:\n",
    "                        correct_chord_predictions += 1\n",
    "                    l_chord_split = l_split[i].split(':')\n",
    "                    p_chord_split = p_split[i].split(':')\n",
    "                    if l_chord_split[0] == p_chord_split[0]:\n",
    "                        correct_root_predictions += 1\n",
    "            elif tokenizer_name == 'GCTSymbolTokenizer':\n",
    "                if '[' in l_split[i]:\n",
    "                    total_chord_predictions += 1\n",
    "                    if p_split[i] == l_split[i]:\n",
    "                        correct_chord_predictions += 1\n",
    "                    l_chord_split = l_split[i][1:].split('x')\n",
    "                    p_chord_split = p_split[i][1:].split('x')\n",
    "                    if l_chord_split[0] == p_chord_split[0]:\n",
    "                        correct_root_predictions += 1\n",
    "            elif tokenizer_name == 'RootTypeTokenizer' or tokenizer_name == 'GCTRootTypeTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to root\n",
    "                    i += 1\n",
    "                    tmp_correct_root = False\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i]:\n",
    "                            correct_root_predictions += 1\n",
    "                            tmp_correct_root = True\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i] and tmp_correct_root:\n",
    "                            correct_chord_predictions += 1\n",
    "            elif tokenizer_name == 'RootPCTokenizer' or tokenizer_name == 'GCTRootPCTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to root\n",
    "                    i += 1\n",
    "                    tmp_correct_root = False\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i]:\n",
    "                            correct_root_predictions += 1\n",
    "                            tmp_correct_root = True\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    while i < len(l_split):\n",
    "                        if l_split[i] == '<bar>' or 'position_' in l_split[i]:\n",
    "                            # already gone too far\n",
    "                            i -= 1\n",
    "                            break\n",
    "                        l_chord_buffer.append( l_split[i] )\n",
    "                        p_chord_buffer.append( p_split[i] )\n",
    "                        i += 1\n",
    "                    # check if type is the same\n",
    "                    if set(l_chord_buffer).issubset( p_chord_buffer ) and tmp_correct_root:\n",
    "                        correct_chord_predictions += 1\n",
    "                    # reset buffers\n",
    "                    l_chord_buffer = []\n",
    "                    p_chord_buffer = []\n",
    "            elif tokenizer_name == 'PitchClassTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    while i < len(l_split):\n",
    "                        if l_split[i] == '<bar>' or 'position_' in l_split[i]:\n",
    "                            # already gone too far\n",
    "                            i -= 1\n",
    "                            break\n",
    "                        l_chord_buffer.append( l_split[i] )\n",
    "                        p_chord_buffer.append( p_split[i] )\n",
    "                        i += 1\n",
    "                    # check if type is the same\n",
    "                    if set(l_chord_buffer).issubset( p_chord_buffer ):\n",
    "                        correct_chord_predictions += 1\n",
    "                    # reset buffers\n",
    "                    l_chord_buffer = []\n",
    "                    p_chord_buffer = []\n",
    "            i += 1\n",
    "    results = {\n",
    "        'correct_bar_predictions': correct_bar_predictions/total_bars,\n",
    "        'correct_new_chord_predictions': correct_new_chord_predictions/total_new_chord_predictions,\n",
    "        'correct_position_predictions': correct_position_predictions/total_position_predictions,\n",
    "        'correct_chord_predictions': correct_chord_predictions/total_chord_predictions,\n",
    "        'correct_root_predictions': correct_root_predictions/total_chord_predictions\n",
    "    }\n",
    "    return results\n",
    "# end symbol_basic_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizers = {\n",
    "#     'ChordSymbolTokenizer': symbol_basic_eval,\n",
    "#     'RootTypeTokenizer': symbol_basic_eval,\n",
    "#     'PitchClassTokenizer': symbol_basic_eval,\n",
    "#     'RootPCTokenizer': symbol_basic_eval,\n",
    "#     'GCTRootPCTokenizer': symbol_basic_eval,\n",
    "#     'GCTSymbolTokenizer': symbol_basic_eval,\n",
    "#     'GCTRootTypeTokenizer': symbol_basic_eval\n",
    "# }\n",
    "\n",
    "tokenizers = {\n",
    "    'ChordSymbolTokenizer': symbol_basic_eval,\n",
    "    'RootTypeTokenizer': symbol_basic_eval,\n",
    "    'PitchClassTokenizer': symbol_basic_eval,\n",
    "    'RootPCTokenizer': symbol_basic_eval,\n",
    "    # 'GCTRootPCTokenizer': symbol_basic_eval,\n",
    "    # 'GCTSymbolTokenizer': symbol_basic_eval,\n",
    "    # 'GCTRootTypeTokenizer': symbol_basic_eval\n",
    "}\n",
    "\n",
    "tokenized_folder = 'tokenized/gen/'\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenizer_name in tokenizers.keys():\n",
    "    if tokenizers[tokenizer_name] is not None:\n",
    "        df = pd.read_csv( tokenized_folder + tokenizer_name + '.csv' )\n",
    "        results[tokenizer_name] = tokenizers[tokenizer_name](df, tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct_bar_predictions': 0.9309529632346947, 'correct_new_chord_predictions': 0.9066162751758121, 'correct_position_predictions': 0.8530354494570157, 'correct_chord_predictions': 0.5773812371429938, 'correct_root_predictions': 0.6157489355594891}\n",
      "{'correct_bar_predictions': 0.9272845846580255, 'correct_new_chord_predictions': 0.9175238004114242, 'correct_position_predictions': 0.8596373726259389, 'correct_chord_predictions': 0.554756908790571, 'correct_root_predictions': 0.5951604982365284}\n",
      "{'correct_bar_predictions': 0.9038884812912693, 'correct_new_chord_predictions': 0.9021671530402334, 'correct_position_predictions': 0.8523656891355308, 'correct_chord_predictions': 0.5971870066497632, 'correct_root_predictions': 0.6479931110366933}\n",
      "{'correct_bar_predictions': 0.9197847884568354, 'correct_new_chord_predictions': 0.8997751518920729, 'correct_position_predictions': 0.8514567286992297, 'correct_chord_predictions': 0.5841266803808066, 'correct_root_predictions': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(results['ChordSymbolTokenizer'])\n",
    "# print(results['GCTSymbolTokenizer'])\n",
    "print(results['RootTypeTokenizer'])\n",
    "# print(results['GCTRootTypeTokenizer'])\n",
    "print(results['RootPCTokenizer'])\n",
    "# print(results['GCTRootPCTokenizer'])\n",
    "print(results['PitchClassTokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ChordSymbolTokenizer': {'correct_bar_predictions': 0.9309529632346947, 'correct_new_chord_predictions': 0.9066162751758121, 'correct_position_predictions': 0.8530354494570157, 'correct_chord_predictions': 0.5773812371429938, 'correct_root_predictions': 0.6157489355594891}, 'RootTypeTokenizer': {'correct_bar_predictions': 0.9272845846580255, 'correct_new_chord_predictions': 0.9175238004114242, 'correct_position_predictions': 0.8596373726259389, 'correct_chord_predictions': 0.554756908790571, 'correct_root_predictions': 0.5951604982365284}, 'PitchClassTokenizer': {'correct_bar_predictions': 0.9197847884568354, 'correct_new_chord_predictions': 0.8997751518920729, 'correct_position_predictions': 0.8514567286992297, 'correct_chord_predictions': 0.5841266803808066, 'correct_root_predictions': 0.0}, 'RootPCTokenizer': {'correct_bar_predictions': 0.9038884812912693, 'correct_new_chord_predictions': 0.9021671530402334, 'correct_position_predictions': 0.8523656891355308, 'correct_chord_predictions': 0.5971870066497632, 'correct_root_predictions': 0.6479931110366933}}\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "with open('results/basic_eval_result.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ChordSymbolTokenizer': {'correct_bar_predictions': 0.9357626151463275, 'correct_new_chord_predictions': 0.9121657178395446, 'correct_position_predictions': 0.8528440893651629, 'correct_chord_predictions': 0.5800602784289336, 'correct_root_predictions': 0.6184758168683921}, 'RootTypeTokenizer': {'correct_bar_predictions': 0.9335615880003261, 'correct_new_chord_predictions': 0.9156101994928958, 'correct_position_predictions': 0.8605463330622398, 'correct_chord_predictions': 0.5612304120719674, 'correct_root_predictions': 0.6004732354122951}, 'PitchClassTokenizer': {'correct_bar_predictions': 0.9103285236814217, 'correct_new_chord_predictions': 0.9058029947854375, 'correct_position_predictions': 0.8553796105822131, 'correct_chord_predictions': 0.5976175668564321, 'correct_root_predictions': 0.0}, 'RootPCTokenizer': {'correct_bar_predictions': 0.9241053232249123, 'correct_new_chord_predictions': 0.9010668325120796, 'correct_position_predictions': 0.8503085681481127, 'correct_chord_predictions': 0.5798689183370808, 'correct_root_predictions': 0.6518203128737502}}\n"
     ]
    }
   ],
   "source": [
    "tokenized_folder = 'tokenized/gen_reg/'\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tokenizer_name in tokenizers.keys():\n",
    "    if tokenizers[tokenizer_name] is not None:\n",
    "        df = pd.read_csv( tokenized_folder + tokenizer_name + '.csv' )\n",
    "        results[tokenizer_name] = tokenizers[tokenizer_name](df, tokenizer_name=tokenizer_name)\n",
    "\n",
    "print(results)\n",
    "with open('results/basic_eval_reg_result.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
