{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_basic_eval(df, tokenizer_name='ChordSymbolTokenizer'):\n",
    "    total_pieces = len(df['labels'])\n",
    "\n",
    "    correct_bar_predictions = 0\n",
    "    total_bars = 0\n",
    "\n",
    "    correct_new_chord_predictions = 0\n",
    "    total_new_chord_predictions = 0\n",
    "\n",
    "    correct_position_predictions = 0\n",
    "    total_position_predictions = 0\n",
    "\n",
    "    correct_chord_predictions = 0\n",
    "    correct_root_predictions = 0\n",
    "    total_chord_predictions = 0\n",
    "\n",
    "    for p_i in range(total_pieces):\n",
    "        l = df['labels'].iloc[p_i]\n",
    "        p = df['predictions'].iloc[p_i]\n",
    "        # for each token that should have been predicted\n",
    "        l_split = l.split(' ')\n",
    "        p_split = p.split(' ')\n",
    "        # arm root and type for RootType tokenizers\n",
    "        arm_root = False\n",
    "        tmp_root = None\n",
    "        tmp_type = None\n",
    "        # keep a chord buffer to accumulate elements of chords for \n",
    "        # non single-word chord representations\n",
    "        l_chord_buffer = []\n",
    "        p_chord_buffer = []\n",
    "        i = 0\n",
    "        while i < len (l_split):\n",
    "            # how many bars were correctly predicted\n",
    "            if l_split[i] == '<bar>':\n",
    "                total_bars += 1\n",
    "                arm_root = False\n",
    "                if p_split[i] == '<bar>':\n",
    "                    correct_bar_predictions += 1\n",
    "            # how many new chords were correctly predicted\n",
    "            if 'position_' in l_split[i]:\n",
    "                total_new_chord_predictions += 1\n",
    "                arm_root = True\n",
    "                if 'position_' in p_split[i]:\n",
    "                    correct_new_chord_predictions += 1\n",
    "            # how many correct positions were predicted\n",
    "            if 'position_' in l_split[i]:\n",
    "                total_position_predictions += 1\n",
    "                if p_split[i] == l_split[i]:\n",
    "                    correct_position_predictions += 1\n",
    "            # how many exact chords and roots were predicted\n",
    "            if tokenizer_name == 'ChordSymbolTokenizer':\n",
    "                if ':' in l_split[i]:\n",
    "                    total_chord_predictions += 1\n",
    "                    if p_split[i] == l_split[i]:\n",
    "                        correct_chord_predictions += 1\n",
    "                    l_chord_split = l_split[i].split(':')\n",
    "                    p_chord_split = p_split[i].split(':')\n",
    "                    if l_chord_split[0] == p_chord_split[0]:\n",
    "                        correct_root_predictions += 1\n",
    "            elif tokenizer_name == 'GCTSymbolTokenizer':\n",
    "                if '[' in l_split[i]:\n",
    "                    total_chord_predictions += 1\n",
    "                    if p_split[i] == l_split[i]:\n",
    "                        correct_chord_predictions += 1\n",
    "                    l_chord_split = l_split[i][1:].split('x')\n",
    "                    p_chord_split = p_split[i][1:].split('x')\n",
    "                    if l_chord_split[0] == p_chord_split[0]:\n",
    "                        correct_root_predictions += 1\n",
    "            elif tokenizer_name == 'RootTypeTokenizer' or tokenizer_name == 'GCTRootTypeTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to root\n",
    "                    i += 1\n",
    "                    tmp_correct_root = False\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i]:\n",
    "                            correct_root_predictions += 1\n",
    "                            tmp_correct_root = True\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i] and tmp_correct_root:\n",
    "                            correct_chord_predictions += 1\n",
    "            elif tokenizer_name == 'RootPCTokenizer' or tokenizer_name == 'GCTRootPCTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to root\n",
    "                    i += 1\n",
    "                    tmp_correct_root = False\n",
    "                    if i < len(l_split):\n",
    "                        if p_split[i] == l_split[i]:\n",
    "                            correct_root_predictions += 1\n",
    "                            tmp_correct_root = True\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    while i < len(l_split):\n",
    "                        if l_split[i] == '<bar>' or 'position_' in l_split[i]:\n",
    "                            # already gone too far\n",
    "                            i -= 1\n",
    "                            break\n",
    "                        l_chord_buffer.append( l_split[i] )\n",
    "                        p_chord_buffer.append( p_split[i] )\n",
    "                        i += 1\n",
    "                    # check if type is the same\n",
    "                    if set(l_chord_buffer).issubset( p_chord_buffer ) and tmp_correct_root:\n",
    "                        correct_chord_predictions += 1\n",
    "                    # reset buffers\n",
    "                    l_chord_buffer = []\n",
    "                    p_chord_buffer = []\n",
    "            elif tokenizer_name == 'PitchClassTokenizer':\n",
    "                if arm_root:\n",
    "                    total_chord_predictions += 1\n",
    "                    # progress to type\n",
    "                    i += 1\n",
    "                    while i < len(l_split):\n",
    "                        if l_split[i] == '<bar>' or 'position_' in l_split[i]:\n",
    "                            # already gone too far\n",
    "                            i -= 1\n",
    "                            break\n",
    "                        l_chord_buffer.append( l_split[i] )\n",
    "                        p_chord_buffer.append( p_split[i] )\n",
    "                        i += 1\n",
    "                    # check if type is the same\n",
    "                    if set(l_chord_buffer).issubset( p_chord_buffer ):\n",
    "                        correct_chord_predictions += 1\n",
    "                    # reset buffers\n",
    "                    l_chord_buffer = []\n",
    "                    p_chord_buffer = []\n",
    "            i += 1\n",
    "    results = {\n",
    "        'correct_bar_predictions': correct_bar_predictions/total_bars,\n",
    "        'correct_new_chord_predictions': correct_new_chord_predictions/total_new_chord_predictions,\n",
    "        'correct_position_predictions': correct_position_predictions/total_position_predictions,\n",
    "        'correct_chord_predictions': correct_chord_predictions/total_chord_predictions,\n",
    "        'correct_root_predictions': correct_root_predictions/total_chord_predictions\n",
    "    }\n",
    "    return results\n",
    "# end symbol_basic_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    'ChordSymbolTokenizer': symbol_basic_eval,\n",
    "    'RootTypeTokenizer': symbol_basic_eval,\n",
    "    'PitchClassTokenizer': symbol_basic_eval,\n",
    "    'RootPCTokenizer': symbol_basic_eval,\n",
    "    'GCTRootPCTokenizer': symbol_basic_eval,\n",
    "    'GCTSymbolTokenizer': symbol_basic_eval,\n",
    "    'GCTRootTypeTokenizer': symbol_basic_eval\n",
    "}\n",
    "\n",
    "tokenized_folder = 'tokenized/gen/'\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenizer_name in tokenizers.keys():\n",
    "    if tokenizers[tokenizer_name] is not None:\n",
    "        df = pd.read_csv( tokenized_folder + tokenizer_name + '.csv' )\n",
    "        results[tokenizer_name] = tokenizers[tokenizer_name](df, tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct_bar_predictions': 0.9731942575145806, 'correct_new_chord_predictions': 0.7508786103817411, 'correct_position_predictions': 0.7477681276509796, 'correct_chord_predictions': 0.11949101191678449, 'correct_root_predictions': 0.19495051504746516}\n",
      "{'correct_bar_predictions': 0.9344997756841633, 'correct_new_chord_predictions': 0.7916380529186023, 'correct_position_predictions': 0.7857402544940416, 'correct_chord_predictions': 0.16068453051114615, 'correct_root_predictions': 0.335014636343166}\n",
      "{'correct_bar_predictions': 0.8400628084342755, 'correct_new_chord_predictions': 0.8660068672995355, 'correct_position_predictions': 0.857685316097758, 'correct_chord_predictions': 0.08616753267202015, 'correct_root_predictions': 0.1985514092268934}\n",
      "{'correct_bar_predictions': 0.9253909439176035, 'correct_new_chord_predictions': 0.7858537378996177, 'correct_position_predictions': 0.779834051899455, 'correct_chord_predictions': 0.11371791333149324, 'correct_root_predictions': 0.221757817120776}\n",
      "{'correct_bar_predictions': 0.8540264692687304, 'correct_new_chord_predictions': 0.8789739446576449, 'correct_position_predictions': 0.8736012926681479, 'correct_chord_predictions': 0.2921833972934761, 'correct_root_predictions': 0.5002221773379115}\n",
      "{'correct_bar_predictions': 0.8111821444593988, 'correct_new_chord_predictions': 0.8796202787315693, 'correct_position_predictions': 0.8744496061401738, 'correct_chord_predictions': 0.25150474651585536, 'correct_root_predictions': 0.40004039587962026}\n"
     ]
    }
   ],
   "source": [
    "print(results['ChordSymbolTokenizer'])\n",
    "print(results['GCTSymbolTokenizer'])\n",
    "print(results['RootTypeTokenizer'])\n",
    "print(results['GCTRootTypeTokenizer'])\n",
    "print(results['RootPCTokenizer'])\n",
    "print(results['GCTRootPCTokenizer'])\n",
    "print(results['PitchClassTokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ChordSymbolTokenizer': {'correct_bar_predictions': 0.9731942575145806, 'correct_new_chord_predictions': 0.7508786103817411, 'correct_position_predictions': 0.7477681276509796, 'correct_chord_predictions': 0.11949101191678449, 'correct_root_predictions': 0.19495051504746516}, 'RootTypeTokenizer': {'correct_bar_predictions': 0.8400628084342755, 'correct_new_chord_predictions': 0.8660068672995355, 'correct_position_predictions': 0.857685316097758, 'correct_chord_predictions': 0.08616753267202015, 'correct_root_predictions': 0.1985514092268934}, 'RootPCTokenizer': {'correct_bar_predictions': 0.8540264692687304, 'correct_new_chord_predictions': 0.8789739446576449, 'correct_position_predictions': 0.8736012926681479, 'correct_chord_predictions': 0.2921833972934761, 'correct_root_predictions': 0.5002221773379115}, 'GCTRootPCTokenizer': {'correct_bar_predictions': 0.8111821444593988, 'correct_new_chord_predictions': 0.8796202787315693, 'correct_position_predictions': 0.8744496061401738, 'correct_chord_predictions': 0.25150474651585536, 'correct_root_predictions': 0.40004039587962026}, 'GCTSymbolTokenizer': {'correct_bar_predictions': 0.9344997756841633, 'correct_new_chord_predictions': 0.7916380529186023, 'correct_position_predictions': 0.7857402544940416, 'correct_chord_predictions': 0.16068453051114615, 'correct_root_predictions': 0.335014636343166}, 'GCTRootTypeTokenizer': {'correct_bar_predictions': 0.9253909439176035, 'correct_new_chord_predictions': 0.7858537378996177, 'correct_position_predictions': 0.779834051899455, 'correct_chord_predictions': 0.11371791333149324, 'correct_root_predictions': 0.221757817120776}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
